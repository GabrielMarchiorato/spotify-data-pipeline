# Spotify Data Pipeline ðŸŽ¶

Pipeline de dados completa utilizando **Airflow + Spark + MinIO + Postgres + Streamlit**.

## Arquitetura

* **IngestÃ£o**: Spotify API â†’ MinIO (bronze)
* **TransformaÃ§Ã£o**: Bronze â†’ Silver (Parquet no MinIO) usando Spark
* **Carga**: Silver â†’ Postgres (DW) usando Spark JDBC
* **VisualizaÃ§Ã£o**: Streamlit para dashboards interativos

## Estrutura do Projeto

```
spotify-data-pipeline/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .env.example
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ README.md
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ dags/
â”‚       â”œâ”€â”€ spotify_pipeline.py
â”‚       â””â”€â”€ scripts/
â”‚           â””â”€â”€ etl_ingest.py
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ etl_transform.py
â”‚   â””â”€â”€ etl_load.py
â”œâ”€â”€ streamlit/
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ docker/
    â””â”€â”€ postgres/
        â””â”€â”€ init.sql
```

## ConfiguraÃ§Ã£o Inicial

1. Copie `.env.example` para `.env` e preencha com suas credenciais do Spotify API.
2. Ajuste as variÃ¡veis de Postgres e MinIO, se necessÃ¡rio.

## Como Rodar

No terminal, dentro da pasta do projeto:

```bash
docker compose up -d
```

### UIs:

* Airflow: [http://localhost:8080](http://localhost:8080) (login: admin/admin)
* MinIO Console: [http://localhost:9001](http://localhost:9001) (login: minio/minio123)
* Spark Master UI: [http://localhost:8081](http://localhost:8081)
* Streamlit: [http://localhost:8501](http://localhost:8501)

## Passo a Passo

### 1. IngestÃ£o (Airflow)

No Airflow, DAG `spotify_pipeline`:

* Rode `check_env` para validar variÃ¡veis de ambiente.
* Execute `ingest_spotify_to_minio` para coletar dados das playlists e salvar no MinIO (bronze).

### 2. TransformaÃ§Ã£o (Spark)

Execute dentro do container `spark-master`:

```bash
docker compose exec spark-master bash -lc '
  spark-submit --master spark://spark-master:7077 \
  --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.postgresql:postgresql:42.7.4 \
  --conf spark.hadoop.fs.s3a.endpoint=${MINIO_ENDPOINT#http://} \
  --conf spark.hadoop.fs.s3a.access.key=$MINIO_ROOT_USER \
  --conf spark.hadoop.fs.s3a.secret.key=$MINIO_ROOT_PASSWORD \
  --conf spark.hadoop.fs.s3a.path.style.access=true \
  --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
  /opt/spark/apps/etl_transform.py --bucket $MINIO_BUCKET'
```

Isso cria a camada Silver (`tracks`, `artists`, `track_artists`, `audio_features`).

### 3. Carga (Silver â†’ Postgres)

```bash
docker compose exec spark-master bash -lc '
  spark-submit --master spark://spark-master:7077 \
  --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.postgresql:postgresql:42.7.4 \
  --conf spark.hadoop.fs.s3a.endpoint=${MINIO_ENDPOINT#http://} \
  --conf spark.hadoop.fs.s3a.access.key=$MINIO_ROOT_USER \
  --conf spark.hadoop.fs.s3a.secret.key=$MINIO_ROOT_PASSWORD \
  --conf spark.hadoop.fs.s3a.path.style.access=true \
  --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
  /opt/spark/apps/etl_load.py --bucket $MINIO_BUCKET --jdbc_url $JDBC_URL --jdbc_user $JDBC_USER --jdbc_password $JDBC_PASSWORD'
```

### 4. VisualizaÃ§Ã£o (Streamlit)

Acesse [http://localhost:8501](http://localhost:8501) para explorar os dashboards com contagens, top faixas e anÃ¡lises de popularidade.

## ObservaÃ§Ãµes

* VariÃ¡veis de ambiente sensÃ­veis **nÃ£o devem ser commitadas**. Use `.env` local.
* Spark lÃª dados do MinIO e grava no Postgres via JDBC.
* Airflow Ã© usado para orquestraÃ§Ã£o da ingestÃ£o e checagem de variÃ¡veis.
* Streamlit consome o Postgres para visualizaÃ§Ãµes interativas.

---
